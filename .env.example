# Example environment variables for mt5-mcp-ui
# Copy this file to .env and fill in your values

# ===== Application Mode =====
# Controls UI presentation: development, production, or demo
# - development: Full settings access (default)
# - production: Settings hidden, environment variables only
# - demo: Settings visible but read-only
APP_MODE=development
PRODUCTION_MODE=false  # Legacy: set to true for production mode

# ===== MCP Server Connection (REQUIRED) =====
# This UI is a CLIENT ONLY - must connect to MetaTrader 5 MCP Server
# Default: Testing server (no MT5 installation needed)
MCP_URL=https://unapposable-nondiscriminatingly-mona.ngrok-free.dev/gradio_api/mcp
MT5_MCP_URL=https://unapposable-nondiscriminatingly-mona.ngrok-free.dev/gradio_api/mcp  # Alternative (legacy)
MCP_TRANSPORT=streamable_http  # 'sse' or 'streamable_http'

# For SSE transport, add /sse to URL:
# MCP_URL=https://unapposable-nondiscriminatingly-mona.ngrok-free.dev/gradio_api/mcp/sse
# MCP_TRANSPORT=sse

# For local MCP server (requires main project on Windows with MT5):
# MCP_URL=http://localhost:7860/gradio_api/mcp
# MCP_TRANSPORT=streamable_http

# ===== LLM Provider Configuration =====
# UI dropdown providers (set at least one):

# Option 1: OpenAI (default)
LLM_PROVIDER=openai
LLM_MODEL=gpt-4o-mini
OPENAI_API_KEY=your-openai-api-key
# LLM_API_KEY=your-openai-api-key  # Alternative

# Option 2: Azure OpenAI (Cognitive Services)
# LLM_PROVIDER=azure_openai
# LLM_MODEL=gpt-4o  # or your deployment name (e.g., gpt-4o-mini, gpt-35-turbo)
# AZURE_OPENAI_API_KEY=your-azure-openai-key
# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
# LLM_API_VERSION=2024-12-01-preview

# Option 3: Azure AI Foundry (DeepSeek, Phi, Mistral)
# LLM_PROVIDER=azure_foundry
# LLM_MODEL=DeepSeek-R1-0528
# AZURE_AI_ENDPOINT=https://your-resource.services.ai.azure.com/
# AZURE_AI_API_KEY=your-azure-ai-key

# Option 4: Azure AI Inference SDK
# LLM_PROVIDER=azure_ai_inference
# LLM_MODEL=your-model-name
# AZURE_AI_API_KEY=your-azure-ai-key
# LLM_BASE_URL=https://your-resource.services.ai.azure.com/

# Option 5: Ollama (local)
# LLM_PROVIDER=ollama
# LLM_MODEL=llama3.2
# LLM_BASE_URL=http://localhost:11434/v1

# For other providers (Anthropic, Google, xAI, etc.), use openai provider:
# LLM_PROVIDER=openai
# LLM_BASE_URL=https://api.anthropic.com/v1  # or other provider URL
# LLM_API_KEY=your-provider-api-key
# LLM_MODEL=claude-3-5-sonnet-20241022

# For Azure OpenAI via OpenAI-compatible endpoint:
# LLM_PROVIDER=openai
# LLM_BASE_URL=https://your-resource.cognitiveservices.azure.com/openai/v1/
# LLM_API_KEY=your-azure-api-key
# LLM_MODEL=your-deployment-name  # e.g., gpt-4o-mini, gpt-5-mini (custom)

# ===== System Prompt (Optional) =====
# Custom system prompt for the AI analyst
# SYSTEM_PROMPT="You are a professional financial analyst..."

# ===== Gradio Server Configuration =====
GRADIO_SERVER_PORT=7860
GRADIO_SERVER_NAME=0.0.0.0  # 0.0.0.0 for LAN access, 127.0.0.1 for local only
GRADIO_SHARE=false  # Set to true for public Gradio link

# ===== HuggingFace Spaces (Optional) =====
# Auto-detected when deploying to HuggingFace Spaces
# HF_TOKEN=your-huggingface-token
